\section{Introduction}
In the recent years, there has been some interest in the design and analysis of scheduling algorithms for task sets with intra-task parallelism, or parallel real-time tasks~\cite{li2014federated, baruah2015federated, ueter2018reservation, saifullah2013multi, lakshmanan2010scheduling, baruah2015global, li2013, li2014analysis, bonifaci2013feasibility, andersson2012analyzing, baruah2014improved, serrano2016response}. A parallel real-time task is typically represented as a directed acyclic graph (DAG). Each node in the DAG represents release, execution, and completion of one thread.  A DAG model is often used to model many real-world applications like autonomous vehicles, computer vision, and real-time hybrid testing. The parallel real-time schedulers like federated ~\cite{li2014federated, baruah2015federated, ueter2018reservation} and global schedulers~\cite{ saifullah2013multi, lakshmanan2010scheduling, baruah2015global} facilitate the real-time completion of tasks with execution demands greater than its deadlines by allowing a task to run on multiple cores.  

IIn real-time systems, shared resources like cache memory complicate the calculation of the calculation of the worst-case execution time and schedulability of a task-set.  There are several existing works that take into account the effects of cache memory on worst-case execution time and schedulability tests~\cite{Tomiyama:2000, tan:2007, ee:1998, altmeyer:2011, negi:2003, altmeyer:2012, Li:2009, zhang2016cache, chattopadhyay2014cache, Calandrino:2009, tessler:2016, tessler:2018}. Most of the previous works~\cite{Tomiyama:2000, tan:2007, ee:1998, altmeyer:2011, negi:2003, altmeyer:2012, Li:2009, zhang2016cache, chattopadhyay2014cache} consider the negative impacts of cache memory such as eviction of instructions caches by a preempting task.  This paper focuses on the positive impacts of caches.

Works like BUNDLE~\cite{tessler:2016} and BUNDLEP~\cite{ tessler:2018} look into the positive impact of caches like inter-thread cache benefit for a sequential task. The BUNDLE scheduling algorithm partitions a code segment into multiple conflict-free regions and each region is allocated a bundle of threads. Only one bundle can execute at a time, and hence, instructions loaded into the cache block by the first thread in the bundle can be re-used by the second thread. Thus, BUNDLE makes use of inter-thread cache benefit to reduce the worst-case execution time of a task. For a parallel real-time task, each node represents the execution of one thread and not multiple threads, and hence, BUNDLE scheduler is not directly applicable to a parallel real-time task.  

In this paper, we address the problem of maximizing inter-thread cache benefit for parallel real-time tasks executing on a federated scheduler.  To the best of our knowledge, this is the first work to consider inter-thread cache benefit for a parallel real-time task. This paper proposes the concept of a collapse to enable inter-thread cache benefit. In the proposed method, multiple nodes that represent the execution of a thread on the same code segment are collapsed into one node. The collapsed node represents the execution of multiple threads.  By collapsing two nodes into one node, the federated scheduler proposed in~\cite{li2014federated} executes both the threads on the same core, sequentially. Thus, instructions loaded into the cache block by the first thread can be re-used by the second thread. 

A pair of nodes that represent the execution of the same code segment cannot always be collapsed into a node. A collapse can violate the constraints of the scheduler, for example, introduce loops into the parallel task. In some cases, a collapse can render the task not schedulable. For example, consider a DAG task that consists of precisely two nodes (that run in parallel) and its utilization is greater than $1$.  This task is schedulable only when both nodes run in parallel. If the two nodes are collapsed and executed serially, then the task may not be schedulable. Therefore, two nodes can be collapsed only when they meet specific criteria. This paper presents the required conditions for the collapse of two nodes in a DAG task with execution demand greater than the deadline. Since this is the first work on inter-thread cache benefit for parallel real-time tasks, we consider that two nodes under consideration for collapse are at the same depth from the first node. In summary, the following contributions are made in this paper.
\begin{itemize}
	\item This paper introduces the concept of collapse for a DAG task, where two distinct nodes that execute the same cache block can be collapsed into one node. To accommodate the collapse of two nodes, the paper also proposes a new DAG task model.
	\item This paper introduces the necessary and sufficient condition for two nodes in a DAG task to be called a candidate.
	\item This paper introduces the condition to determine if the collapse of two nodes in a DAG task, with utilization greater than $1$, is beneficial for the task-set or not.
\end{itemize}

The rest of the paper is organized as follows. Section~\ref{sec:related} presents the related work. Section~\ref{sec:sysmodel} presents the task model. Section~\ref{sec:collapsedef} presents the concept of collapse. Section~\ref{sec:candidacy} presents the necessary and sufficient condition for a pair of nodes to be called a candidate for collapse. Section~\ref{sec:collapse-bound} presents the definition for a beneficial collapse and presents the condition for a beneficial collapse assuming that all instruction in the node fit entirely in the cache block. Section~\ref{} presents the condition for a beneficial collapse when assuming all instructions of a node do not fit entirely in the cache block. Section~\ref{} presents the evaluation to show the inter-thread cache benefit for parallel real-time tasks. Section~\ref{} presents the conclusion and future work.