\clearpage
\section{System Model}

\subsection{Directed Acyclic Graph Model}

The directed acyclic graph (DAG) model of hard real-time
tasks~\cite{li2014analysis} defines the set of ${n}$ sporadic tasks
${\tau}$ as ${\tau = \{\tau_1,\tau_2, ..., \tau_n\}}$. A task
${\tau_i \in \tau}$ generates a potentially infinite number
of jobs, each arriving no less than ${T_i}$ time units after the
previous job with deadline ${D_i}$ constrained by the task's period
${D_i \leq T_i}$. Each parallel task ${\tau_i \in \tau}$ is
represented by a DAG ${G_i}$. The set of ${n}$ DAGs is denoted
${\mathbb{G} = \{G_1, G_2, ..., G_n\}}$.

Each node ${v \in V_i}$ of the DAG ${G_i = (V_i, E_i)}$ represents the 
execution of a thread. A thread executes on exactly one of the ${m}$
identical cores of the target architecture or distributed
system. Associated with each node ${v}$ is a worst-case execution time 
(WCET) ${c_v}$; an upper bound on the time required for the thread to
complete execution without interruption on one core.

An edge ${e = (u,v)}$ in ${E_i}$ indicates an execution dependency
between ${u,v \in V_i}$. For ${v}$ to begin execution on any of the
${m}$ cores, all immediate predecessors ${\{u|(u,v) \in E_i\}}$ must
have completed execution.

A DAG ${G_i}$ is required to have a source and sink node,
${s,t \in V_i}$ respectively. The source has no incoming edges,
${\not \exists u~|~(u,s) \in E_i}$. The sink has no outgoing edges,
${\not \exists v~|~(t,v) \in E_i}$. When a job is released, execution
begins with one thread of ${s}$ on one of the ${m}$ cores. The job
terminates when the one thread of ${t}$ completes on one of the ${m}$
cores. During the execution of a job, up to ${m}$ cores may execute
any of the ${v \in V}$ threads in parallel.

An example DAG task is shown in Figure~\ref{fig:dag}. The node
${v}$ represents the execution of a single thread with a WCET
${c_v = 20}$ time units. The directed edge ${(u,v)}$ restricts ${v}$
from executing on any core until ${u}$ has completed.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.4\columnwidth]{dag}
  \caption{An Example of a Parallel Real-time Task}
  \label{fig:dag}
\end{figure}

For a DAG ${G_i = (V_i, E_i)}$, the \emph{critical path} ${\lambda_i}$
is the path from the source ${s}$ to sink ${t}$ with the greatest sum
of WCETs for nodes along the path among all paths from ${s}$ to
${t}$. The \emph{critical path length} ${L_i}$ is the sum of WCET
values for all nodes ${v}$ on the critical path ${v \in \lambda_i}$.
The \emph{workload} of ${G_i}$ is the sum of all WCET values
${v \in V_i}$.

\begin{definition}[Critical Path Length of ${G_i}$]
  \begin{equation}
    L_i = \sum_{v \in \lambda_i} c_v
  \end{equation}
\end{definition}

\begin{definition}[Workload of ${G_i}$]
  \begin{equation}
    C_i = \sum_{v \in V_i} c_v
  \end{equation}
\end{definition}

In Figure~\ref{fig:dag}, the critical path is
${\lambda = \langle u, v, t \rangle}$. The DAG has a critical path
length of ${L = 60}$ and a workload ${C = 70}$.

\subsection{Federated Scheduler}
For a task set $\tau$ , the federated scheduling algorithm proposed in~\cite{li2014analysis} works as
follows. The task sets is divided into two disjoint sets
$\tau_{high}$  and $\tau_{low}$. $\tau_{high}$ contains all tasks with
high utilization (i.e. $u_i > 1$) and $\tau_{low}$ contains all
remaining low utilization tasks. Each task in $\tau_{high}$ is
assigned $m_{i}$ dedicated cores (no other task is executed on these
cores), where: \begin{equation}\label{eq:m} m_{i} = \left\lceil \frac{C_{i} - L_{i}}{D_{i} - L_{i}}
\right\rceil \end{equation}

\noindent The total
number of cores assigned to high-utilization tasks is denoted as $m_{high} = \sum_{\tau_{i} \in \tau{high}} m_{i}$. 
The remaining cores to all low-utilization tasks in $\tau{low}$, denoted
as ${m_{low} =  \mathbb{m} - m_{high}}$. The federated scheduling algorithm admits
the task set ${\tau}$, if $m_{low}$ is non-negative and all tasks in
$\tau_{low}$ are schedulable sequentially.  

After a valid core allocation, runtime scheduling proceeds as
follows. Any greedy (work-conserving) parallel scheduler can be used
to schedule a high-utilization task $\tau_{i} \in \tau_{high}$ on its
assigned $m_{i}$ cores. Informally, a greedy scheduler is one that never
keeps a core idle if some node is ready to execute. 

All low-utilization tasks are treated and executed as though they are
sequential tasks and any multiprocessor scheduling algorithm (such as
partitioned EDF, or various rate-monotonic schedulers) can be used to
schedule all the low-utilization tasks on the allocated $m_{low}$
cores. The federated scheduler treats all low-utilization tasks as sequential tasks
since $C_{i} \le D_{i}$ and parallel execution is not required to meet their
deadlines. 

\subsection{Processing Model}

In this work, for each core, we assume a dedicated direct mapped
instruction cache. We assume a time-compositional architecture\addcite,
where memory and execution demand are separable. Copying a block of 
main memory to cache memory requires $\brt$ cycles, commonly
referred to as the the block reload time (BRT). If multiple cores share
the same processing platform their cache contents do not interfere with
one another. The impact of a shared cache between cores is not considered.


\subsection{Proposed Changes to the Directed Acyclic Graph Model}
%For the rest of the paper, we work with one DAG task at a time, and hence we drop the subscript notation.
For a DAG ${G_i = (V_i, E_i)}$ representing a parallel task, each node ${v \in V_i}$ represents
the release, execution, and termination of a single thread within one task 
\cite{li2014analysis}. In the existing model, the only relationship between thread releases and the executable object they execute is the worst-case execution time of the node. Two nodes ${u, v \in V_i}$ may represent two threads executing the same object (possibly on different processors).

To take advantage of instruction cache reuse, we propose a simple modification to the DAG
model. Where possible, distinct nodes that represent the execution of
the same object are collapsed into a single node. To accommodate collapse,
nodes are identified by their executable object, and a new attribute is added to every node
indicating the number of threads which will be executed over the object.

In the existing directed acyclic graph model~\cite{li2014analysis}, each node ${v \in V_i}$ is characterized
with a single worst case execution time for a single thread. We
propose that each node's WCET is characterized by a function
${c_{v}(\eta_{v})}$ where $\eta_{v}$ is the number of threads that will execute the
node ${v}$ on the same core serially (one after another) with no
other thread executing a different object on the core in between executions.

In this work, a node ${v \in V_i}$ is represented by a tuple ${v = \langle \alpha_{v}, c_{v}(\eta_{v}), \eta_{v} \rangle }$, where $\alpha_{v}$ is the code segment the thread executes, $\eta_{v}$ is the number of threads, and ${c_{v}}$ is the WCET function for ${\eta_v}$ threads. The proposed model is  compatible with the previous model \cite{li2014analysis} where the previous model can be expressed as ${v = <\alpha_{v}, c_{v}(\eta_v), \eta_v=1>}$. Note that, we will discuss the computation of $c_{v}(\eta_{v})$ in a later section.

A node $v \in V_i$ is said to be dependent on $u \in V_i$ , if there is a path from $u$ to $v$ of some arbitrary length in the DAG $G_i$. This dependency is expressed as \textbf{${u \prec^* v}$}, where $\prec^*$ indicates the dependency and $*$ denotes an arbitrary length. Similarly, a path between two nodes $u, v \in V$ of a DAG $G_i$ is represented by $u \prec^{x} v$, where $x$ denotes the length of the path. For example, $u \prec^{3} v$ denotes that there exists a path from $u$ to $v$ of length of 3.


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}{
      \includegraphics[width=\textwidth]{existingDAGmodel}
      \caption{Existing DAG model}
      \label{fig:existingDAGmodel}
    }
  \end{subfigure} \quad
  \begin{subfigure}[b]{0.4\textwidth}{
      \includegraphics[width=\textwidth]{proposedDAGmodel}
      \caption{Proposed DAG model}
      \label{fig:proposedDAGmodel}
    }
  \end{subfigure}
  \caption{Proposed changes to the DAG model}
  \label{fig:dag-change}
\end{figure}


Fig.~\ref{fig:dag-change} highlights the proposed changes to the DAG task model.  Node $v$ in Fig.~\ref{fig:existingDAGmodel} represents a single thread execution which requires a worst-case execution time of $20$ time units. In the proposed model, we represent node $v$ as a tuple $v : \langle \alpha_v = B, c_v(\eta_v) = 20,  \eta_v = 1\rangle$, where $B$ refers to a unique code segment, $ c_v(\eta_v)$ refers to the worst-case execution time of $1$ thread which is $20$ time units, and $\eta_v$ refers to the number of threads which is 1.

 A goal of this work is to bring the inter-thread cache benefit \addcite to parallel DAG tasks. 

%% ct - commented out because splitting a code segment into multiple code segments may introduce loops in the DAG. 
%%		For example, if a loop is too big and cannot fit into a cache block then we it will be split across multiple segments which will introduce a DAG.
%%
%%A goal of this work is to bring the inter-thread cache benefit \addcite to parallel DAG tasks. As a first-step, two requirements are placed on nodes of the graph.
%%\begin{description}
%%\item[R1] All executable objects must fit entirely within the cache.
%%\item[R2] No two instructions of an executable object may evict one another.
%%\end{description}
%%
%%Requirements R1 and R2 may be met for any executable object by repeatedly dividing
%%the object source that result in objects larger than the cache into separate code segments, carefully recompiling those code segments to maximize cache use, and replacing the original node with a serial set of nodes.
%%\\
%%\\
%%\emph{ct-3 A figure is needed to illustrate the transformation from one over-sized node, to multiple correctly sized nodes}
%%\\



%%Given a timing-compositional architecture with the restrictions \textbf{R1} and \textbf{R2},
%%${c_i(n)}$ can be expressed for any node in terms of the memory demand of all instructions
%%of ${v_j \in V_i}$ into the cache ${\gamma_j}$ and the worst-case execution demand
%%to execute the node assuming all instructions of ${v_j}$ have
%%been cached ${\iota_j}$. Equation~\ref{eq:c_i} is an expression for
%%${c_i(n)}$. 
%% 
%%\begin{equation}
%%  \label{eq:c_i}
%%  c_i(n) = \begin{cases}
%%    0, & n \le 0 \\
%%    \gamma_j + \iota_j \cdot n, & n > 0
%%  \end{cases}
%%\end{equation}
%%
%%For a node ${v_i \in V_j}$, the upper bound of memory demand of the node is denoted ${\gamma_i}$. It is the number of cycles required to load all blocks of the node. The complete set of blocks of the node are equivalent to the evicting cache blocks (ECBs)\addcite [Tan \& Mooney] of the object. Thus, the memory demand is the product of the BRT and count of ECBs of the node ${\textsc{ecb}_i}$ found in Equation~\ref{eq:mem-demand}.
%%
%%\begin{equation}{\label{eq:mem-demand}}
%%    \gamma_i = \mathbb{B} \cdot \textsc{ecb}_i
%%\end{equation}
%%
%%The execution demand ${\iota_i}$ for a node ${v_i^j \in V_i}$ is the worst case execution time of a single thread given all instructions are present in the cache. Any suitable WCET calculation method {\addcite} may be used to calculate the value.

%%\section{Problem Formulation}
